[{"id":"5c74502c-62fc-4f6e-a455-063c7543b78f","timestamp":"2024-08-05T05:08:11.045Z","changes":[{"count":2,"removed":true,"value":"[]"},{"count":4102,"added":true,"value":"# What is an Open Table Format Why to use one.\n\nNov 14, 2023 · 8 min read\n1. Introduction\n2. What is an Open Table Format (OTF)\n3. Why use an Open Table Format (OTF)\n3.0. Setup\n3.1. Evolve data and partition schema without reprocessing\n3.2. See previous point-in-time table state, aka time travel\n3.3. Git like branches & tags for your tables\n3.4. Handle multiple reads & writes concurrently\n4. Conclusion\n5. Further reading\n6. References\n1. Introduction\nIf you are in the data space, you might have heard of open table formats such as Apache Iceberg, Apache Hudi, or Delta Lake. If you are wondering:\n\nWhat is an open table format? & How is it different from file formats like Parquet or ORC?\nWhat are the pros and cons of using an open table format? & How does it work?\nIs it just a pointer to some metadata files and helps you sift through the data quickly?\nThen this post is for you. Understanding the underlying principles behind open table formats will enable you to deeply understand what happens behind the scenes and make the right decisions when designing your data systems.\n\nThis post will review what open table formats are, their main benefits, and some examples with Apache Iceberg. By the end of this post, you will know what OTFs are, why you use them, and how they work.\n\n2. What is an Open Table Format (OTF)\nTL;DR Open table formats are wrappers around your data store & uses a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nHere is an architecture comparison of Apache Iceberg, Apache Hudi & Delta Lake (ref ).\n\nArchitecture Comparison\n\nNote: When we refer to metadata in the following sections, we refer to the files that store information about DMLs, DDL, and column statistics. Every OTF has its naming conventions.\n\n3. Why use an Open Table Format (OTF)\nWe will use Apache Iceberg to illustrate the benefits of using an OTF. While there are differences in implementation and minor differences, the below sections also apply to Apache Hudi and Delta format.\n\n3.0. Setup\nFollowing along with the code is recommended! Please follow the setup section in this GitHub repo to create and insert data into the orders table.\n\nApache Iceberg can work with multiple data processing systems; we use spark-sql in our examples.\n\n3.1. Evolve data and partition schema without reprocessing\nApache Iceberg enables you to change your table’s data schema or partition schema without reprocessing the existing data. The metadata files track schema & partition changes, allowing the systems to process data using the appropriate data/partition schema for the corresponding historical data.\n\nSchema and partition evolution are common operations in analytical tables and are often expensive or error-prone with traditional OLAP systems. Apache Iceberg makes data and partition schema evolution a simple task.\n\nFor example:\n`\n-- schema evolution\nALTER TABLE local.warehouse.orders ALTER COLUMN cust_id TYPE bigint;\nALTER TABLE local.warehouse.orders DROP COLUMN order_status;\n\n-- parititon evolution\nALTER TABLE local.warehouse.orders ADD PARTITION FIELD cust_id;\nINSERT INTO local.warehouse.orders VALUES \n('e481f51cbdc54678b7cc49136f2d6af7',69,CAST('2023-11-14 09:56:33' AS TIMESTAMP)),\n('e481f51cbdc54678b7cc49136f2d6af7',87,CAST('2023-11-14 10:56:33' AS TIMESTAMP));\n\n-- check snapshots\nselect committed_at, snapshot_id, manifest_list from local.warehouse.orders.snapshots order by committed_at desc;\n-- We will have two since we had two insert statements\n\n-- See the partitions column statistics and data files added per snapshot\nselect added_snapshot_id, added_data_files_count, partition_summaries from local.warehouse.orders.manifests;\nThe above code shows that each operation on the table (set of Inserts/Deletes/Updates) will be considered a snapshot. We can also see the partition schema, column statistics, & number of files added or deleted per snapshot via the table’s manifests.\n\nWe can also open the file ./data/iceberg-warehouse/warehouse/orders/metadata/v<the latest number>.metadata.json (in your project directory) to see the different data and partition schemas under the “schemas” and “partition-specs” sections, respectively.\n\nA visual representation of how systems work with data whose partitions have evolved: Partition evolution\n\nHidden partition lets you define(& enable use of) partitions based on column transformations. E.g., you may want to partition by date on a timestamp column. In HIVE (without OTFs), you must create a separate date column in the data. However, with Apache Iceberg, you can define a partition on a transformation of another column, as shown below:\n```sql\n/* -- created in the setup section\nCREATE TABLE local.warehouse.orders (\n    order_id string,\n    cust_id INT,\n    order_status string,\n    order_date timestamp\n) USING iceberg\nPARTITIONED BY (date(order_date));\n*/\n  \n-- The below query automatically uses the partition to prune data files to scan.\nSELECT cust_id, order_date FROM local.warehouse.orders WHERE order_date BETWEEN '2023-11-01 12:45:33' AND '2023-11-03 12:45:33';\n3.2. See previous point-in-time table state, aka time travel\nSince metadata files track all the changes in data (& data/partition schema), we can return to a point-in-time table state(aka time travel). E.g., If you want to see what a table looked like four days ago, you can do this using time travel.\n\n-- get the time of the first data snapshot\nselect min(committed_at) as min_committed_at from local.warehouse.orders.snapshots;\n-- e.g. 2023-11-21 12:03:08.833\n\n-- Query data as of the oldest committed_at (min_committed_at from the above query) time or after\nSELECT * FROM local.warehouse.orders TIMESTAMP AS OF '2023-11-21 12:03:08.833';\n-- 15 rows\n\n-- Query without time travel, and you will see all the rows\nSELECT * FROM local.warehouse.orders;\n-- 17 rows\nApache Iceberg Time Travel\n````\nNote: For Apache Iceberg, as your data size grows, it’s recommended to clean up old snapshots .\n\n3.3. Git like branches & tags for your tables\nApache Iceberg enables the branching of tables by managing isolated metadata files per branch. For example, assume you have to run a pipeline in production for\n\n a week to validate that the changes you made are valid. You can do this using branches as shown below:\n\nBranching\n\nDROP TABLE IF EXISTS local.warehouse.orders_agg;\nCREATE TABLE local.warehouse.orders_agg(\n    order_date date,\n    num_orders int\n)  USING iceberg;\nINSERT INTO local.warehouse.orders_agg\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-02' GROUP BY 1;\n\n-- Create two branches that are both stored for ten days\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v1` RETAIN 10 DAYS;\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v2` RETAIN 10 DAYS;\n\n-- Use different logic for each of the branches\n\n-- inserting into branch v1\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- inserting into branch v2\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- validate data, the v2 logic is correct\nselect * from local.warehouse.orders_agg.`branch_branch-v1` order by order_date;\nselect * from local.warehouse.orders_agg.`branch_branch-v2` order by order_date;\nFrom the above exercise, we notice that branch-v2 has the correct logic, so we fast forward the main branch to branch-v2. The main branch will now have the accurate data for the past two days.\n\nselect * from local.warehouse.orders_agg order by order_date desc; \n-- Push the main branch to branch v2's state\nCALL local.system.fast_forward('warehouse.orders_agg', 'main', 'branch-v2');\nselect * from local.warehouse.orders_agg order by order_date desc;\n3.4. Handle multiple reads & writes concurrently\nIn traditional OLAP systems (e.g., HIVE), if multiple processes read/write to the same table without proper safeguards, there may be inconsistent data reads, or data may get overwritten during writes. Apache Iceberg atomically updates its metadata, which forces writers to “commit” their changes one at a time (if multiple writers collide, there will be a retry for the failed writer ).\n\nWhen reading data, Apache Iceberg uses the most recent snapshot (using a metadata file) to ensure that no in-process data operations impact reads.\n\nSince Apache Icerber is OSS, we can use any system that implements the table format to be able to read and write.\n\nE.g., We can use DuckDB to read our data. Exit your spark shell with exit; and docker with exit. Start DuckDB CLI with the duckdb command via your terminal.\n\nINSTALL iceberg;\nLOAD iceberg;\n\n-- Count orders by date\nWITH orders as (SELECT * FROM iceberg_scan('data/iceberg-warehouse/warehouse/orders', ALLOW_MOVED_PATHS=true))\nselect strftime(order_date, '%Y-%m-%d') as order_date\n, count(distinct order_id) as num_orders\nfrom orders \ngroup by strftime(order_date, '%Y-%m-%d') \norder by 1 desc;\n4. Conclusion\nTo recap, Open table formats are wrappers around your data store & use a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nThe next time you work with OTFs, remember that it’s a system of metadata files, storing every change to the data, & statistical information about data files. OTFs can significantly improve how you manage analytical tables and developer experience.\n\nIf you have any questions or comments, please leave them in the comment section below.\n\n5. Further reading\nComparing Apache Hudi, Apache Iceberg, Delta Lake\nZ-Ordering\nParquet\nPartitioning\nGit branch & tag\nApache Iceberg Inspecting tables\nPartition Transformation functions\nSchema on read v schema on write\n6. References\nApache Iceberg Docs\nDremio Blog\nIf you found this article helpful, share it with a friend or colleague using one of the socials below!\n\ntest\n"}],"message":"New"},{"id":"3a93dbb7-1687-4ff2-8116-4b9f313eb980","timestamp":"2024-08-05T05:10:03.831Z","changes":[{"value":"# What is an Open Table Format Why to use one.\n\nNov 14, 2023 · 8 min read\n1. Introduction\n2. What is an Open Table Format (OTF)\n3. Why use an Open Table Format (OTF)\n3.0. Setup\n3.1. Evolve data and partition schema without reprocessing\n3.2. See previous point-in-time table state, aka time travel\n3.3. Git like branches & tags for your tables\n3.4. Handle multiple reads & writes concurrently\n4. Conclusion\n5. Further reading\n6. References\n1. Introduction\nIf you are in the data space, you might have heard of open table formats such as Apache Iceberg, Apache Hudi, or Delta Lake. If you are wondering:\n\nWhat is an open table format? & How is it different from file formats like Parquet or ORC?\nWhat are the pros and cons of using an open table format? & How does it work?\nIs it just a pointer to some metadata files and helps you sift through the data quickly?\nThen this post is for you. Understanding the underlying principles behind open table formats will enable you to deeply understand what happens behind the scenes and make the right decisions when designing your data systems.\n\nThis post will review what open table formats are, their main benefits, and some examples with Apache Iceberg. By the end of this post, you will know what OTFs are, why you use them, and how they work.\n\n2. What is an Open Table Format (OTF)\nTL;DR Open table formats are wrappers around your data store & uses a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nHere is an architecture comparison of Apache Iceberg, Apache Hudi & Delta Lake (ref ).\n\nArchitecture Comparison\n\nNote: When we refer to metadata in the following sections, we refer to the files that store information about DMLs, DDL, and column statistics. Every OTF has its naming conventions.\n\n3. Why use an Open Table Format (OTF)\nWe will use Apache Iceberg to illustrate the benefits of using an OTF. While there are differences in implementation and minor differences, the below sections also apply to Apache Hudi and Delta format.\n\n3.0. Setup\nFollowing along with the code is recommended! Please follow the setup section in this GitHub repo to create and insert data into the orders table.\n\nApache Iceberg can work with multiple data processing systems; we use spark-sql in our examples.\n\n3.1. Evolve data and partition schema without reprocessing\nApache Iceberg enables you to change your table’s data schema or partition schema without reprocessing the existing data. The metadata files track schema & partition changes, allowing the systems to process data using the appropriate data/partition schema for the corresponding historical data.\n\nSchema and partition evolution are common operations in analytical tables and are often expensive or error-prone with traditional OLAP systems. Apache Iceberg makes data and partition schema evolution a simple task.\n\nFor example:\n`\n-- schema evolution\nALTER TABLE local.warehouse.orders ALTER COLUMN cust_id TYPE bigint;\nALTER TABLE local.warehouse.orders DROP COLUMN order_status;\n\n-- parititon evolution\nALTER TABLE local.warehouse.orders ADD PARTITION FIELD cust_id;\nINSERT INTO local.warehouse.orders VALUES \n('e481f51cbdc54678b7cc49136f2d6af7',69,CAST('2023-11-14 09:56:33' AS TIMESTAMP)),\n('e481f51cbdc54678b7cc49136f2d6af7',87,CAST('2023-11-14 10:56:33' AS TIMESTAMP));\n\n-- check snapshots\nselect committed_at, snapshot_id, manifest_list from local.warehouse.orders.snapshots order by committed_at desc;\n-- We will have two since we had two insert statements\n\n-- See the partitions column statistics and data files added per snapshot\nselect added_snapshot_id, added_data_files_count, partition_summaries from local.warehouse.orders.manifests;\nThe above code shows that each operation on the table (set of Inserts/Deletes/Updates) will be considered a snapshot. We can also see the partition schema, column statistics, & number of files added or deleted per snapshot via the table’s manifests.\n\nWe can also open the file ./data/iceberg-warehouse/warehouse/orders/metadata/v<the latest number>.metadata.json (in your project directory) to see the different data and partition schemas under the “schemas” and “partition-specs” sections, respectively.\n\nA visual representation of how systems work with data whose partitions have evolved: Partition evolution\n\nHidden partition lets you define(& enable use of) partitions based on column transformations. E.g., you may want to partition by date on a timestamp column. In HIVE (without OTFs), you must create a separate date column in the data. However, with Apache Iceberg, you can define a partition on a transformation of another column, as shown below:\n```sql\n/* -- created in the setup section\nCREATE TABLE local.warehouse.orders (\n    order_id string,\n    cust_id INT,\n    order_status string,\n    order_date timestamp\n) USING iceberg\nPARTITIONED BY (date(order_date));\n*/\n  \n-- The below query automatically uses the partition to prune data files to scan.\nSELECT cust_id, order_date FROM local.warehouse.orders WHERE order_date BETWEEN '2023-11-01 12:45:33' AND '2023-11-03 12:45:33';\n3.2. See previous point-in-time table state, aka time travel\nSince metadata files track all the changes in data (& data/partition schema), we can return to a point-in-time table state(aka time travel). E.g., If you want to see what a table looked like four days ago, you can do this using time travel.\n\n-- get the time of the first data snapshot\nselect min(committed_at) as min_committed_at from local.warehouse.orders.snapshots;\n-- e.g. 2023-11-21 12:03:08.833\n\n-- Query data as of the oldest committed_at (min_committed_at from the above query) time or after\nSELECT * FROM local.warehouse.orders TIMESTAMP AS OF '2023-11-21 12:03:08.833';\n-- 15 rows\n\n-- Query without time travel, and you will see all the rows\nSELECT * FROM local.warehouse.orders;\n-- 17 rows\nApache Iceberg Time Travel\n````\nNote: For Apache Iceberg, as your data size grows, it’s recommended to clean up old snapshots .\n\n3.3. Git like branches & tags for your tables\nApache Iceberg enables the branching of tables by managing isolated metadata files per branch. For example, assume you have to run a pipeline in production for\n\n a week to validate that the changes you made are valid. You can do this using branches as shown below:\n\nBranching\n\nDROP TABLE IF EXISTS local.warehouse.orders_agg;\nCREATE TABLE local.warehouse.orders_agg(\n    order_date date,\n    num_orders int\n)  USING iceberg;\nINSERT INTO local.warehouse.orders_agg\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-02' GROUP BY 1;\n\n-- Create two branches that are both stored for ten days\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v1` RETAIN 10 DAYS;\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v2` RETAIN 10 DAYS;\n\n-- Use different logic for each of the branches\n\n-- inserting into branch v1\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- inserting into branch v2\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- validate data, the v2 logic is correct\nselect * from local.warehouse.orders_agg.`branch_branch-v1` order by order_date;\nselect * from local.warehouse.orders_agg.`branch_branch-v2` order by order_date;\nFrom the above exercise, we notice that branch-v2 has the correct logic, so we fast forward the main branch to branch-v2. The main branch will now have the accurate data for the past two days.\n\nselect * from local.warehouse.orders_agg order by order_date desc; \n-- Push the main branch to branch v2's state\nCALL local.system.fast_forward('warehouse.orders_agg', 'main', 'branch-v2');\nselect * from local.warehouse.orders_agg order by order_date desc;\n3.4. Handle multiple reads & writes concurrently\nIn traditional OLAP systems (e.g., HIVE), if multiple processes read/write to the same table without proper safeguards, there may be inconsistent data reads, or data may get overwritten during writes. Apache Iceberg atomically updates its metadata, which forces writers to “commit” their changes one at a time (if multiple writers collide, there will be a retry for the failed writer ).\n\nWhen reading data, Apache Iceberg uses the most recent snapshot (using a metadata file) to ensure that no in-process data operations impact reads.\n\nSince Apache Icerber is OSS, we can use any system that implements the table format to be able to read and write.\n\nE.g., We can use DuckDB to read our data. Exit your spark shell with exit; and docker with exit. Start DuckDB CLI with the duckdb command via your terminal.\n\nINSTALL iceberg;\nLOAD iceberg;\n\n-- Count orders by date\nWITH orders as (SELECT * FROM iceberg_scan('data/iceberg-warehouse/warehouse/orders', ALLOW_MOVED_PATHS=true))\nselect strftime(order_date, '%Y-%m-%d') as order_date\n, count(distinct order_id) as num_orders\nfrom orders \ngroup by strftime(order_date, '%Y-%m-%d') \norder by 1 desc;\n4. Conclusion\nTo recap, Open table formats are wrappers around your data store & use a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nThe next time you work with OTFs, remember that it’s a system of metadata files, storing every change to the data, & statistical information about data files. OTFs can significantly improve how you manage analytical tables and developer experience.\n\nIf you have any questions or comments, please leave them in the comment section below.\n\n5. Further reading\nComparing Apache Hudi, Apache Iceberg, Delta Lake\nZ-Ordering\nParquet\nPartitioning\nGit branch & tag\nApache Iceberg Inspecting tables\nPartition Transformation functions\nSchema on read v schema on write\n6. References\nApache Iceberg Docs\nDremio Blog\nIf you found this article helpful, share it with a friend or colleague using one of the socials below!\n\ntest\n"}],"message":"Test"},{"id":"9ea46bba-f3a7-4330-ac69-8726c413e3a9","timestamp":"2024-08-05T05:10:04.193Z","changes":[{"value":"# What is an Open Table Format Why to use one.\n\nNov 14, 2023 · 8 min read\n1. Introduction\n2. What is an Open Table Format (OTF)\n3. Why use an Open Table Format (OTF)\n3.0. Setup\n3.1. Evolve data and partition schema without reprocessing\n3.2. See previous point-in-time table state, aka time travel\n3.3. Git like branches & tags for your tables\n3.4. Handle multiple reads & writes concurrently\n4. Conclusion\n5. Further reading\n6. References\n1. Introduction\nIf you are in the data space, you might have heard of open table formats such as Apache Iceberg, Apache Hudi, or Delta Lake. If you are wondering:\n\nWhat is an open table format? & How is it different from file formats like Parquet or ORC?\nWhat are the pros and cons of using an open table format? & How does it work?\nIs it just a pointer to some metadata files and helps you sift through the data quickly?\nThen this post is for you. Understanding the underlying principles behind open table formats will enable you to deeply understand what happens behind the scenes and make the right decisions when designing your data systems.\n\nThis post will review what open table formats are, their main benefits, and some examples with Apache Iceberg. By the end of this post, you will know what OTFs are, why you use them, and how they work.\n\n2. What is an Open Table Format (OTF)\nTL;DR Open table formats are wrappers around your data store & uses a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nHere is an architecture comparison of Apache Iceberg, Apache Hudi & Delta Lake (ref ).\n\nArchitecture Comparison\n\nNote: When we refer to metadata in the following sections, we refer to the files that store information about DMLs, DDL, and column statistics. Every OTF has its naming conventions.\n\n3. Why use an Open Table Format (OTF)\nWe will use Apache Iceberg to illustrate the benefits of using an OTF. While there are differences in implementation and minor differences, the below sections also apply to Apache Hudi and Delta format.\n\n3.0. Setup\nFollowing along with the code is recommended! Please follow the setup section in this GitHub repo to create and insert data into the orders table.\n\nApache Iceberg can work with multiple data processing systems; we use spark-sql in our examples.\n\n3.1. Evolve data and partition schema without reprocessing\nApache Iceberg enables you to change your table’s data schema or partition schema without reprocessing the existing data. The metadata files track schema & partition changes, allowing the systems to process data using the appropriate data/partition schema for the corresponding historical data.\n\nSchema and partition evolution are common operations in analytical tables and are often expensive or error-prone with traditional OLAP systems. Apache Iceberg makes data and partition schema evolution a simple task.\n\nFor example:\n`\n-- schema evolution\nALTER TABLE local.warehouse.orders ALTER COLUMN cust_id TYPE bigint;\nALTER TABLE local.warehouse.orders DROP COLUMN order_status;\n\n-- parititon evolution\nALTER TABLE local.warehouse.orders ADD PARTITION FIELD cust_id;\nINSERT INTO local.warehouse.orders VALUES \n('e481f51cbdc54678b7cc49136f2d6af7',69,CAST('2023-11-14 09:56:33' AS TIMESTAMP)),\n('e481f51cbdc54678b7cc49136f2d6af7',87,CAST('2023-11-14 10:56:33' AS TIMESTAMP));\n\n-- check snapshots\nselect committed_at, snapshot_id, manifest_list from local.warehouse.orders.snapshots order by committed_at desc;\n-- We will have two since we had two insert statements\n\n-- See the partitions column statistics and data files added per snapshot\nselect added_snapshot_id, added_data_files_count, partition_summaries from local.warehouse.orders.manifests;\nThe above code shows that each operation on the table (set of Inserts/Deletes/Updates) will be considered a snapshot. We can also see the partition schema, column statistics, & number of files added or deleted per snapshot via the table’s manifests.\n\nWe can also open the file ./data/iceberg-warehouse/warehouse/orders/metadata/v<the latest number>.metadata.json (in your project directory) to see the different data and partition schemas under the “schemas” and “partition-specs” sections, respectively.\n\nA visual representation of how systems work with data whose partitions have evolved: Partition evolution\n\nHidden partition lets you define(& enable use of) partitions based on column transformations. E.g., you may want to partition by date on a timestamp column. In HIVE (without OTFs), you must create a separate date column in the data. However, with Apache Iceberg, you can define a partition on a transformation of another column, as shown below:\n```sql\n/* -- created in the setup section\nCREATE TABLE local.warehouse.orders (\n    order_id string,\n    cust_id INT,\n    order_status string,\n    order_date timestamp\n) USING iceberg\nPARTITIONED BY (date(order_date));\n*/\n  \n-- The below query automatically uses the partition to prune data files to scan.\nSELECT cust_id, order_date FROM local.warehouse.orders WHERE order_date BETWEEN '2023-11-01 12:45:33' AND '2023-11-03 12:45:33';\n3.2. See previous point-in-time table state, aka time travel\nSince metadata files track all the changes in data (& data/partition schema), we can return to a point-in-time table state(aka time travel). E.g., If you want to see what a table looked like four days ago, you can do this using time travel.\n\n-- get the time of the first data snapshot\nselect min(committed_at) as min_committed_at from local.warehouse.orders.snapshots;\n-- e.g. 2023-11-21 12:03:08.833\n\n-- Query data as of the oldest committed_at (min_committed_at from the above query) time or after\nSELECT * FROM local.warehouse.orders TIMESTAMP AS OF '2023-11-21 12:03:08.833';\n-- 15 rows\n\n-- Query without time travel, and you will see all the rows\nSELECT * FROM local.warehouse.orders;\n-- 17 rows\nApache Iceberg Time Travel\n````\nNote: For Apache Iceberg, as your data size grows, it’s recommended to clean up old snapshots .\n\n3.3. Git like branches & tags for your tables\nApache Iceberg enables the branching of tables by managing isolated metadata files per branch. For example, assume you have to run a pipeline in production for\n\n a week to validate that the changes you made are valid. You can do this using branches as shown below:\n\nBranching\n\nDROP TABLE IF EXISTS local.warehouse.orders_agg;\nCREATE TABLE local.warehouse.orders_agg(\n    order_date date,\n    num_orders int\n)  USING iceberg;\nINSERT INTO local.warehouse.orders_agg\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-02' GROUP BY 1;\n\n-- Create two branches that are both stored for ten days\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v1` RETAIN 10 DAYS;\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v2` RETAIN 10 DAYS;\n\n-- Use different logic for each of the branches\n\n-- inserting into branch v1\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- inserting into branch v2\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- validate data, the v2 logic is correct\nselect * from local.warehouse.orders_agg.`branch_branch-v1` order by order_date;\nselect * from local.warehouse.orders_agg.`branch_branch-v2` order by order_date;\nFrom the above exercise, we notice that branch-v2 has the correct logic, so we fast forward the main branch to branch-v2. The main branch will now have the accurate data for the past two days.\n\nselect * from local.warehouse.orders_agg order by order_date desc; \n-- Push the main branch to branch v2's state\nCALL local.system.fast_forward('warehouse.orders_agg', 'main', 'branch-v2');\nselect * from local.warehouse.orders_agg order by order_date desc;\n3.4. Handle multiple reads & writes concurrently\nIn traditional OLAP systems (e.g., HIVE), if multiple processes read/write to the same table without proper safeguards, there may be inconsistent data reads, or data may get overwritten during writes. Apache Iceberg atomically updates its metadata, which forces writers to “commit” their changes one at a time (if multiple writers collide, there will be a retry for the failed writer ).\n\nWhen reading data, Apache Iceberg uses the most recent snapshot (using a metadata file) to ensure that no in-process data operations impact reads.\n\nSince Apache Icerber is OSS, we can use any system that implements the table format to be able to read and write.\n\nE.g., We can use DuckDB to read our data. Exit your spark shell with exit; and docker with exit. Start DuckDB CLI with the duckdb command via your terminal.\n\nINSTALL iceberg;\nLOAD iceberg;\n\n-- Count orders by date\nWITH orders as (SELECT * FROM iceberg_scan('data/iceberg-warehouse/warehouse/orders', ALLOW_MOVED_PATHS=true))\nselect strftime(order_date, '%Y-%m-%d') as order_date\n, count(distinct order_id) as num_orders\nfrom orders \ngroup by strftime(order_date, '%Y-%m-%d') \norder by 1 desc;\n4. Conclusion\nTo recap, Open table formats are wrappers around your data store & use a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nThe next time you work with OTFs, remember that it’s a system of metadata files, storing every change to the data, & statistical information about data files. OTFs can significantly improve how you manage analytical tables and developer experience.\n\nIf you have any questions or comments, please leave them in the comment section below.\n\n5. Further reading\nComparing Apache Hudi, Apache Iceberg, Delta Lake\nZ-Ordering\nParquet\nPartitioning\nGit branch & tag\nApache Iceberg Inspecting tables\nPartition Transformation functions\nSchema on read v schema on write\n6. References\nApache Iceberg Docs\nDremio Blog\nIf you found this article helpful, share it with a friend or colleague using one of the socials below!\n\ntest\n"}],"message":"Test"},{"id":"967c5dbe-dd33-4a0a-abf2-7fecddb9e5e6","timestamp":"2024-08-05T05:10:05.387Z","changes":[{"value":"# What is an Open Table Format Why to use one.\n\nNov 14, 2023 · 8 min read\n1. Introduction\n2. What is an Open Table Format (OTF)\n3. Why use an Open Table Format (OTF)\n3.0. Setup\n3.1. Evolve data and partition schema without reprocessing\n3.2. See previous point-in-time table state, aka time travel\n3.3. Git like branches & tags for your tables\n3.4. Handle multiple reads & writes concurrently\n4. Conclusion\n5. Further reading\n6. References\n1. Introduction\nIf you are in the data space, you might have heard of open table formats such as Apache Iceberg, Apache Hudi, or Delta Lake. If you are wondering:\n\nWhat is an open table format? & How is it different from file formats like Parquet or ORC?\nWhat are the pros and cons of using an open table format? & How does it work?\nIs it just a pointer to some metadata files and helps you sift through the data quickly?\nThen this post is for you. Understanding the underlying principles behind open table formats will enable you to deeply understand what happens behind the scenes and make the right decisions when designing your data systems.\n\nThis post will review what open table formats are, their main benefits, and some examples with Apache Iceberg. By the end of this post, you will know what OTFs are, why you use them, and how they work.\n\n2. What is an Open Table Format (OTF)\nTL;DR Open table formats are wrappers around your data store & uses a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nHere is an architecture comparison of Apache Iceberg, Apache Hudi & Delta Lake (ref ).\n\nArchitecture Comparison\n\nNote: When we refer to metadata in the following sections, we refer to the files that store information about DMLs, DDL, and column statistics. Every OTF has its naming conventions.\n\n3. Why use an Open Table Format (OTF)\nWe will use Apache Iceberg to illustrate the benefits of using an OTF. While there are differences in implementation and minor differences, the below sections also apply to Apache Hudi and Delta format.\n\n3.0. Setup\nFollowing along with the code is recommended! Please follow the setup section in this GitHub repo to create and insert data into the orders table.\n\nApache Iceberg can work with multiple data processing systems; we use spark-sql in our examples.\n\n3.1. Evolve data and partition schema without reprocessing\nApache Iceberg enables you to change your table’s data schema or partition schema without reprocessing the existing data. The metadata files track schema & partition changes, allowing the systems to process data using the appropriate data/partition schema for the corresponding historical data.\n\nSchema and partition evolution are common operations in analytical tables and are often expensive or error-prone with traditional OLAP systems. Apache Iceberg makes data and partition schema evolution a simple task.\n\nFor example:\n`\n-- schema evolution\nALTER TABLE local.warehouse.orders ALTER COLUMN cust_id TYPE bigint;\nALTER TABLE local.warehouse.orders DROP COLUMN order_status;\n\n-- parititon evolution\nALTER TABLE local.warehouse.orders ADD PARTITION FIELD cust_id;\nINSERT INTO local.warehouse.orders VALUES \n('e481f51cbdc54678b7cc49136f2d6af7',69,CAST('2023-11-14 09:56:33' AS TIMESTAMP)),\n('e481f51cbdc54678b7cc49136f2d6af7',87,CAST('2023-11-14 10:56:33' AS TIMESTAMP));\n\n-- check snapshots\nselect committed_at, snapshot_id, manifest_list from local.warehouse.orders.snapshots order by committed_at desc;\n-- We will have two since we had two insert statements\n\n-- See the partitions column statistics and data files added per snapshot\nselect added_snapshot_id, added_data_files_count, partition_summaries from local.warehouse.orders.manifests;\nThe above code shows that each operation on the table (set of Inserts/Deletes/Updates) will be considered a snapshot. We can also see the partition schema, column statistics, & number of files added or deleted per snapshot via the table’s manifests.\n\nWe can also open the file ./data/iceberg-warehouse/warehouse/orders/metadata/v<the latest number>.metadata.json (in your project directory) to see the different data and partition schemas under the “schemas” and “partition-specs” sections, respectively.\n\nA visual representation of how systems work with data whose partitions have evolved: Partition evolution\n\nHidden partition lets you define(& enable use of) partitions based on column transformations. E.g., you may want to partition by date on a timestamp column. In HIVE (without OTFs), you must create a separate date column in the data. However, with Apache Iceberg, you can define a partition on a transformation of another column, as shown below:\n```sql\n/* -- created in the setup section\nCREATE TABLE local.warehouse.orders (\n    order_id string,\n    cust_id INT,\n    order_status string,\n    order_date timestamp\n) USING iceberg\nPARTITIONED BY (date(order_date));\n*/\n  \n-- The below query automatically uses the partition to prune data files to scan.\nSELECT cust_id, order_date FROM local.warehouse.orders WHERE order_date BETWEEN '2023-11-01 12:45:33' AND '2023-11-03 12:45:33';\n3.2. See previous point-in-time table state, aka time travel\nSince metadata files track all the changes in data (& data/partition schema), we can return to a point-in-time table state(aka time travel). E.g., If you want to see what a table looked like four days ago, you can do this using time travel.\n\n-- get the time of the first data snapshot\nselect min(committed_at) as min_committed_at from local.warehouse.orders.snapshots;\n-- e.g. 2023-11-21 12:03:08.833\n\n-- Query data as of the oldest committed_at (min_committed_at from the above query) time or after\nSELECT * FROM local.warehouse.orders TIMESTAMP AS OF '2023-11-21 12:03:08.833';\n-- 15 rows\n\n-- Query without time travel, and you will see all the rows\nSELECT * FROM local.warehouse.orders;\n-- 17 rows\nApache Iceberg Time Travel\n````\nNote: For Apache Iceberg, as your data size grows, it’s recommended to clean up old snapshots .\n\n3.3. Git like branches & tags for your tables\nApache Iceberg enables the branching of tables by managing isolated metadata files per branch. For example, assume you have to run a pipeline in production for\n\n a week to validate that the changes you made are valid. You can do this using branches as shown below:\n\nBranching\n\nDROP TABLE IF EXISTS local.warehouse.orders_agg;\nCREATE TABLE local.warehouse.orders_agg(\n    order_date date,\n    num_orders int\n)  USING iceberg;\nINSERT INTO local.warehouse.orders_agg\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-02' GROUP BY 1;\n\n-- Create two branches that are both stored for ten days\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v1` RETAIN 10 DAYS;\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v2` RETAIN 10 DAYS;\n\n-- Use different logic for each of the branches\n\n-- inserting into branch v1\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- inserting into branch v2\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- validate data, the v2 logic is correct\nselect * from local.warehouse.orders_agg.`branch_branch-v1` order by order_date;\nselect * from local.warehouse.orders_agg.`branch_branch-v2` order by order_date;\nFrom the above exercise, we notice that branch-v2 has the correct logic, so we fast forward the main branch to branch-v2. The main branch will now have the accurate data for the past two days.\n\nselect * from local.warehouse.orders_agg order by order_date desc; \n-- Push the main branch to branch v2's state\nCALL local.system.fast_forward('warehouse.orders_agg', 'main', 'branch-v2');\nselect * from local.warehouse.orders_agg order by order_date desc;\n3.4. Handle multiple reads & writes concurrently\nIn traditional OLAP systems (e.g., HIVE), if multiple processes read/write to the same table without proper safeguards, there may be inconsistent data reads, or data may get overwritten during writes. Apache Iceberg atomically updates its metadata, which forces writers to “commit” their changes one at a time (if multiple writers collide, there will be a retry for the failed writer ).\n\nWhen reading data, Apache Iceberg uses the most recent snapshot (using a metadata file) to ensure that no in-process data operations impact reads.\n\nSince Apache Icerber is OSS, we can use any system that implements the table format to be able to read and write.\n\nE.g., We can use DuckDB to read our data. Exit your spark shell with exit; and docker with exit. Start DuckDB CLI with the duckdb command via your terminal.\n\nINSTALL iceberg;\nLOAD iceberg;\n\n-- Count orders by date\nWITH orders as (SELECT * FROM iceberg_scan('data/iceberg-warehouse/warehouse/orders', ALLOW_MOVED_PATHS=true))\nselect strftime(order_date, '%Y-%m-%d') as order_date\n, count(distinct order_id) as num_orders\nfrom orders \ngroup by strftime(order_date, '%Y-%m-%d') \norder by 1 desc;\n4. Conclusion\nTo recap, Open table formats are wrappers around your data store & use a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nThe next time you work with OTFs, remember that it’s a system of metadata files, storing every change to the data, & statistical information about data files. OTFs can significantly improve how you manage analytical tables and developer experience.\n\nIf you have any questions or comments, please leave them in the comment section below.\n\n5. Further reading\nComparing Apache Hudi, Apache Iceberg, Delta Lake\nZ-Ordering\nParquet\nPartitioning\nGit branch & tag\nApache Iceberg Inspecting tables\nPartition Transformation functions\nSchema on read v schema on write\n6. References\nApache Iceberg Docs\nDremio Blog\nIf you found this article helpful, share it with a friend or colleague using one of the socials below!\n\ntest\n"}],"message":"Test"},{"id":"58c0ba8d-882f-4cf1-b275-25eaf0961487","timestamp":"2024-08-05T05:11:58.752Z","changes":[{"value":"# What is an Open Table Format Why to use one.\n\nNov 14, 2023 · 8 min read\n1. Introduction\n2. What is an Open Table Format (OTF)\n3. Why use an Open Table Format (OTF)\n3.0. Setup\n3.1. Evolve data and partition schema without reprocessing\n3.2. See previous point-in-time table state, aka time travel\n3.3. Git like branches & tags for your tables\n3.4. Handle multiple reads & writes concurrently\n4. Conclusion\n5. Further reading\n6. References\n"},{"value":"1","removed":true},{"value":"7","added":true},{"value":". "},{"value":"Introduction\nIf you are in the data space, you might have heard of open table formats such as Apache Iceberg, Apache Hudi, or Delta Lake. If you are wondering:\n\nWhat is an open table format? & How is it different from file formats like Parquet or ORC?\nWhat are the pros and cons of using an open table format? & How does it work?\nIs it just a pointer to some metadata files and helps you sift through the data quickly?\nThen this post is for you. Understanding the underlying principles behind open table formats will enable you to deeply understand what happens behind the scenes and make the right decisions when designing your data systems.\n\nThis post will review what open table formats are, their main benefits, and some examples with Apache Iceberg. By the end of this post, you will know what OTFs are, why you use them, and how they work.\n\n2. What is an Open Table Format (OTF)\nTL;DR Open table formats are wrappers around your data store & uses a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nHere is an architecture comparison of Apache Iceberg, Apache Hudi & Delta Lake (ref ).\n\nArchitecture Comparison\n\nNote: When we refer to metadata in the following sections, we refer to the files that store information about DMLs, DDL, and column statistics. Every OTF has its naming conventions.\n\n3. Why use an Open Table Format (OTF)\nWe will use Apache Iceberg to illustrate the benefits of using an OTF. While there are differences in implementation and minor differences, the below sections also apply to Apache Hudi and Delta format.\n\n3.0. Setup\nFollowing along with the code is recommended! Please follow the setup section in this GitHub repo to create and insert data into the orders table.\n\nApache Iceberg can work with multiple data processing systems; we use spark-sql in our examples.\n\n3.1. Evolve data and partition schema without reprocessing\nApache Iceberg enables you to change your table’s data schema or partition schema without reprocessing the existing data. The metadata files track schema & partition changes, allowing the systems to process data using the appropriate data/partition schema for the corresponding historical data.\n\nSchema and partition evolution are common operations in analytical tables and are often expensive or error-prone with traditional OLAP systems. Apache Iceberg makes data and partition schema evolution a simple task.\n\nFor example:\n`\n-- schema evolution\nALTER TABLE local.warehouse.orders ALTER COLUMN cust_id TYPE bigint;\nALTER TABLE local.warehouse.orders DROP COLUMN order_status;\n\n-- parititon evolution\nALTER TABLE local.warehouse.orders ADD PARTITION FIELD cust_id;\nINSERT INTO local.warehouse.orders VALUES \n('e481f51cbdc54678b7cc49136f2d6af7',69,CAST('2023-11-14 09:56:33' AS TIMESTAMP)),\n('e481f51cbdc54678b7cc49136f2d6af7',87,CAST('2023-11-14 10:56:33' AS TIMESTAMP));\n\n-- check snapshots\nselect committed_at, snapshot_id, manifest_list from local.warehouse.orders.snapshots order by committed_at desc;\n-- We will have two since we had two insert statements\n\n-- See the partitions column statistics and data files added per snapshot\nselect added_snapshot_id, added_data_files_count, partition_summaries from local.warehouse.orders.manifests;\nThe above code shows that each operation on the table (set of Inserts/Deletes/Updates) will be considered a snapshot. We can also see the partition schema, column statistics, & number of files added or deleted per snapshot via the table’s manifests.\n\nWe can also open the file ./data/iceberg-warehouse/warehouse/orders/metadata/v<the latest number>.metadata.json (in your project directory) to see the different data and partition schemas under the “schemas” and “partition-specs” sections, respectively.\n\nA visual representation of how systems work with data whose partitions have evolved: Partition evolution\n\nHidden partition lets you define(& enable use of) partitions based on column transformations. E.g., you may want to partition by date on a timestamp column. In HIVE (without OTFs), you must create a separate date column in the data. However, with Apache Iceberg, you can define a partition on a transformation of another column, as shown below:\n```sql\n/* -- created in the setup section\nCREATE TABLE local.warehouse.orders (\n    order_id string,\n    cust_id INT,\n    order_status string,\n    order_date timestamp\n) USING iceberg\nPARTITIONED BY (date(order_date));\n*/\n  \n-- The below query automatically uses the partition to prune data files to scan.\nSELECT cust_id, order_date FROM local.warehouse.orders WHERE order_date BETWEEN '2023-11-01 12:45:33' AND '2023-11-03 12:45:33';\n3.2. See previous point-in-time table state, aka time travel\nSince metadata files track all the changes in data (& data/partition schema), we can return to a point-in-time table state(aka time travel). E.g., If you want to see what a table looked like four days ago, you can do this using time travel.\n\n-- get the time of the first data snapshot\nselect min(committed_at) as min_committed_at from local.warehouse.orders.snapshots;\n-- e.g. 2023-11-21 12:03:08.833\n\n-- Query data as of the oldest committed_at (min_committed_at from the above query) time or after\nSELECT * FROM local.warehouse.orders TIMESTAMP AS OF '2023-11-21 12:03:08.833';\n-- 15 rows\n\n-- Query without time travel, and you will see all the rows\nSELECT * FROM local.warehouse.orders;\n-- 17 rows\nApache Iceberg Time Travel\n````\nNote: For Apache Iceberg, as your data size grows, it’s recommended to clean up old snapshots .\n\n3.3. Git like branches & tags for your tables\nApache Iceberg enables the branching of tables by managing isolated metadata files per branch. For example, assume you have to run a pipeline in production for\n\n a week to validate that the changes you made are valid. You can do this using branches as shown below:\n\nBranching\n\nDROP TABLE IF EXISTS local.warehouse.orders_agg;\nCREATE TABLE local.warehouse.orders_agg(\n    order_date date,\n    num_orders int\n)  USING iceberg;\nINSERT INTO local.warehouse.orders_agg\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-02' GROUP BY 1;\n\n-- Create two branches that are both stored for ten days\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v1` RETAIN 10 DAYS;\nALTER TABLE local.warehouse.orders_agg CREATE BRANCH `branch-v2` RETAIN 10 DAYS;\n\n-- Use different logic for each of the branches\n\n-- inserting into branch v1\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v1`\nSELECT date(order_date) as order_date, count(order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- inserting into branch v2\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-03' GROUP BY 1;\n\nINSERT INTO local.warehouse.orders_agg.`branch_branch-v2`\nSELECT date(order_date) as order_date, count(distinct order_id) as num_orders from local.warehouse.orders WHERE date(order_date) = '2023-11-04' GROUP BY 1;\n\n-- validate data, the v2 logic is correct\nselect * from local.warehouse.orders_agg.`branch_branch-v1` order by order_date;\nselect * from local.warehouse.orders_agg.`branch_branch-v2` order by order_date;\nFrom the above exercise, we notice that branch-v2 has the correct logic, so we fast forward the main branch to branch-v2. The main branch will now have the accurate data for the past two days.\n\nselect * from local.warehouse.orders_agg order by order_date desc; \n-- Push the main branch to branch v2's state\nCALL local.system.fast_forward('warehouse.orders_agg', 'main', 'branch-v2');\nselect * from local.warehouse.orders_agg order by order_date desc;\n3.4. Handle multiple reads & writes concurrently\nIn traditional OLAP systems (e.g., HIVE), if multiple processes read/write to the same table without proper safeguards, there may be inconsistent data reads, or data may get overwritten during writes. Apache Iceberg atomically updates its metadata, which forces writers to “commit” their changes one at a time (if multiple writers collide, there will be a retry for the failed writer ).\n\nWhen reading data, Apache Iceberg uses the most recent snapshot (using a metadata file) to ensure that no in-process data operations impact reads.\n\nSince Apache Icerber is OSS, we can use any system that implements the table format to be able to read and write.\n\nE.g., We can use DuckDB to read our data. Exit your spark shell with exit; and docker with exit. Start DuckDB CLI with the duckdb command via your terminal.\n\nINSTALL iceberg;\nLOAD iceberg;\n\n-- Count orders by date\nWITH orders as (SELECT * FROM iceberg_scan('data/iceberg-warehouse/warehouse/orders', ALLOW_MOVED_PATHS=true))\nselect strftime(order_date, '%Y-%m-%d') as order_date\n, count(distinct order_id) as num_orders\nfrom orders \ngroup by strftime(order_date, '%Y-%m-%d') \norder by 1 desc;\n4. Conclusion\nTo recap, Open table formats are wrappers around your data store & use a series of files to\n\nTrack schema/partition (DDL) changes on your table.\nTrack the table’s data files & their column statistics.\nTrack all the Inserts/Updates/Deletes (DML) on your table.\nStoring a chronological series of files with all the DDL and DML statements applied to your table & index of the data file locations enables\n\nSchema & Partition Evolution\nTravelling back in time to a previous table state\nCreating table branches & tagging table state (similar to git)\nHandling multiple reads & writes concurrently\nThe next time you work with OTFs, remember that it’s a system of metadata files, storing every change to the data, & statistical information about data files. OTFs can significantly improve how you manage analytical tables and developer experience.\n\nIf you have any questions or comments, please leave them in the comment section below.\n\n5. Further reading\nComparing Apache Hudi, Apache Iceberg, Delta Lake\nZ-Ordering\nParquet\nPartitioning\nGit branch & tag\nApache Iceberg Inspecting tables\nPartition Transformation functions\nSchema on read v schema on write\n6. References\nApache Iceberg Docs\nDremio Blog\nIf you found this article helpful, share it with a friend or colleague using one of the socials below!\n\ntest\n","removed":true}],"message":"Test"},{"id":"b11ce259-4653-4497-ac28-fa75a98b93c3","timestamp":"2024-08-05T05:11:59.321Z","changes":[{"value":"# What is an Open Table Format Why to use one.\n\nNov 14, 2023 · 8 min read\n1. Introduction\n2. What is an Open Table Format (OTF)\n3. Why use an Open Table Format (OTF)\n3.0. Setup\n3.1. Evolve data and partition schema without reprocessing\n3.2. See previous point-in-time table state, aka time travel\n3.3. Git like branches & tags for your tables\n3.4. Handle multiple reads & writes concurrently\n4. Conclusion\n5. Further reading\n6. References\n7. "}],"message":"Test"},{"id":"561f76ef-0711-4559-83fc-ec5c70618390","timestamp":"2024-08-05T05:11:59.396Z","changes":[{"value":"# What is an Open Table Format Why to use one.\n\nNov 14, 2023 · 8 min read\n1. Introduction\n2. What is an Open Table Format (OTF)\n3. Why use an Open Table Format (OTF)\n3.0. Setup\n3.1. Evolve data and partition schema without reprocessing\n3.2. See previous point-in-time table state, aka time travel\n3.3. Git like branches & tags for your tables\n3.4. Handle multiple reads & writes concurrently\n4. Conclusion\n5. Further reading\n6. References\n7. "}],"message":"Test"},{"id":"83cacd7d-f6aa-4114-a4b4-85270eb70a29","timestamp":"2024-08-05T05:12:02.196Z","changes":[{"value":"# What is an Open Table Format Why to use one.\n\nNov 14, 2023 · 8 min read\n1. Introduction\n2. What is an Open Table Format (OTF)\n3. Why use an Open Table Format (OTF)\n3.0. Setup\n3.1. Evolve data and partition schema without reprocessing\n3.2. See previous point-in-time table state, aka time travel\n3.3. Git like branches & tags for your tables\n3.4. Handle multiple reads & writes concurrently\n4. Conclusion\n5. Further reading\n6. References\n7. "}],"message":"Test"},{"id":"1ea88341-2c4f-4c94-810c-078e05180c17","timestamp":"2024-08-05T05:12:02.325Z","changes":[{"value":"# What is an Open Table Format Why to use one.\n\nNov 14, 2023 · 8 min read\n1. Introduction\n2. What is an Open Table Format (OTF)\n3. Why use an Open Table Format (OTF)\n3.0. Setup\n3.1. Evolve data and partition schema without reprocessing\n3.2. See previous point-in-time table state, aka time travel\n3.3. Git like branches & tags for your tables\n3.4. Handle multiple reads & writes concurrently\n4. Conclusion\n5. Further reading\n6. References\n7. "}],"message":"Test"}]