[{"id":"32d856e5-20db-4209-8005-579476ad1793","timestamp":"2024-08-02T06:34:22.537Z","changes":[{"count":286,"added":true,"value":"# Kubernetes\n\n  \n\n\n**Camera Field**\n\n1. Siana et al: “카메라-based의 object detection은  날씨에 의해서 이미지의 퀄리티가 좌우되고 object detection의 성능이 크게 영향을 받게 된다”\n2. Ponn et al: “SHAP 모델의 개발을 통해 다양한 object algorithm의 성능에 대한 분석과 설명을 가능하게 했다.”\n3. Lee at al: “novel YOLO architecture를 AFC(adaptive frame control)을 통해 리소스 자원에 대한 제약이 있는 임베디드 환경을 효율적으로 (녹화)동작시키는 것을 제안했다”\n\n**LiDAR Field**\n\n1. Meyer et al: “Laser Net을 개발 하여 멀티모달에서 분산된 **3D prediection box를** (?)각 point cloud 내부의 각 점으로 대응 및 예상을 가능하게 하였다”\n2. Shi et al:”Point RCNN을 통해 3D proposal boexs를 spatial point에 대해서 생성하였다. 이때 점들의 좌표는 지역 공간 특징들을 더 잘 학습하기 위해 정규화된다”\n3. Ye et al:”VFE의 개발 "},{"count":9,"added":true,"value":"https://talktato.tistory.com/14"},{"count":462,"added":true,"value":"(https://talktato.tistory.com/14)”.\n4. Fan et al:”이 논문에서는 카메라와 LiDAR 센서퓨전을 사용하여 3D 물체 검출을 연구 하였다. 카메라와 라이다 파이프 라인을 구성한 후, RGB 특징맵을 하향식 뷰 도메인에 투영하여 포인트 클라우드 활성화 맵과 효율적으로 통합합니다.”[https://kyungpyo-kim.github.io/study/centerformer/](https://kyungpyo-kim.github.io/study/centerformer/)\n\nMultimodal Field\n\n1. fdsaf\n2. fsdad\n3. fsd\n4. afs\n\n## 줄거리\n\n**데이터 셋:**\n\n1. LiDAR: KITTI dataset: \n2. IMAGE: KITTI dataset: origial image → image pixel to 640* 640\n\n**방법론:** \n\n1. Point cloud preprocessing: image to depth image\n2. Feature fusion strategy: \n\n> Multimodal data fusion 전략은 크게 다음의 3가지로 분류된다. data-layer fusion, feature fusion, decision-layer fusion.\n> \n1. Image와 depth image를 인풋으로 fusion strategy를 사용하여 the multi-modality data를 합성함.\n\n**실험환경:**\n\n- GPU: Nvidia GTX2070Ti\n- Ram: 32GB RAM\n- OS: Ubuntu20.04 LTS\n- ImageSize: 640*640\n- ImageClass:\n    - 1차: Oringin; tram, misc, cyclist, person(sitting), pedestrain, truck, car and van. → 3 dominant; car, pedestrain, cyclist 로 포함시켜서 진행\n    - 2차: 8개중 3개와 완전히 어긋나는 것을 제외하고 진행.\n- Learning Rate: 0.1\n- GD: SGD 사용\n\n성능 개선: \n\n- 0.03s 에 정확도 상당히 좋음,"}],"message":"New"}]