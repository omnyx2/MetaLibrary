# OpenTableIs,

What is open tabel

v # Review of Arc Paper With DC neural Network 

부제: Neural networks for abstarction and reaasoning: Towards broad generalization in machines.  

논문에 대하여,

해당 논문은 Arc문제는 DreamCoder , PeARL, LLM을 섞어서 문제를 송빙하엿다

## 풀이 절차 분석

해당 논문을 통해서 ARC를 접근하는 다양한 툴과 ARC를 풀어내는 툴에 대한 조합을 어떻게 했는지에 대한 좋은 인사이트를 얻을 수 있었다. 이를 분리해보자면 다음의 측면으로써 분리가 된다.

1. ARC를 Solving하는 기법 - DSL search, DreamCoder, LLM, Ensemble,
2. 각 기법별 impl, 아래 표로 정리
3. ARC 문제의 구별, ARC-easy & ARC-hard



| DSL search | DreamCoder | LLM | Ensemble |
| --- | --- | --- | --- |
| Icecuber | DreamCoder | GPT4 & 3.5 | DC + GPT-4|
| ARGA | | LLaMA-65B |  icecuber + DC +GPT-4|


## 논문에서 알려주는 정보

그외에 논문에서 다른 풀이에 대한 것을 보다 이해하기 편하게 정리해 놓았으며 ARC의 풀이에 대한 좋은 도움을 주어 이를 정리하였다.

### Icecuber 
  
해당 풀이가 생각보다 물건이였다.
1. 142개의 hand-craft solutions를 바탕으로 제작됨
2. Color Normalised가 적용됨
3. pieces는 각 조각 퍼즐임 
3. DAG를 이용하여 각 조각 퍼즐이 합당하게 합쳐지면 Greedy Stacking Solver에 넣고 Final Prediction을 만들어냄


```cpp
for (int depth = 0; depth < q.size(); depth++) {
    while (q[depth].size()) {
      int memi = q[depth].front();
      q[depth].pop();
      if (depth > depth_mem[memi/dags]) continue;
      assert(depth == depth_mem[memi/dags]);

      vector<int> ind(mem.begin()+memi, mem.begin()+memi+dags);
      {
	int ok = 1, maxdepth = -1;
	for (int i = 0; i < dags; i++) {
	  maxdepth = max(maxdepth, (int)dag[i].tiny_node[ind[i]].depth);
	  ok &= dag[i].tiny_node[ind[i]].ispiece;
	}
	if (ok && maxdepth >= depth) {
	  Piece3 p;
	  p.memi = memi;
	  p.depth = depth;
	  pieces.piece.push_back(p);
	}
	if (maxdepth < depth) continue;
      }

      newi_list.clear();

      child_time.start();
      {
	for (int i = 0; i <= train.size(); i++)
	  dag[i].tiny_node[ind[i]].child.legacy(slow_child[i]);
	vector<int> newi(dags), ci(dags); //current index into child[]
	int fi = 0;
	while (1) {
	next_iteration:
	  for (int i = 0; i <= train.size(); i++) {
	    auto&child = slow_child[i];//dag[i].node[ind[i]].child;
	    while (ci[i] < child.size() &&
		   child[ci[i]].first < fi) ci[i]++;
	    if (ci[i] == child.size()) goto finish;

	    int next_available_fi = child[ci[i]].first;
	    if (next_available_fi > fi) {
	      fi = next_available_fi;
	      goto next_iteration;

	    } else {

	      newi[i] = child[ci[i]].second;

	      if (newi[i] == -1) {
		fi++;
		goto next_iteration;
	      }
	    }
	  }
	  newi_list.emplace_back(fi, newi);
	  fi++;
	}
      finish:;
      }
      child_time.stop();

      for (auto&[fi, newi] : newi_list) {
	if (0) {
	  int i = train.size();
	  //auto&child = dag[i].node[ind[i]].child;
	  //auto it = lower_bound(child.begin(), child.end(), make_pair(fi, -1));
	  //if (it == child.end() || it->first != fi) {
	  int to = dag[i].tiny_node.getChild(ind[i], fi);
	  if (to == TinyChildren::None) {
	    string name = dag[i].funcs.getName(fi);
	    if (name.substr(0,4) == "Move") {
	      newi[i] = dag[i].applyFunc(ind[i], fi);
	      if (newi[i] != -1 && out_sizes.size())
		dag[i].applyFunc(newi[i], dag[i].funcs.findfi("embed 1"));
	    } else continue;
	  } else {
	    newi[i] = to; //it->second
	  }
	  if (newi[i] == -1) continue;
	}

	int new_depth = -1;
	for (int i = 0; i < dags; i++) {
	  new_depth = max(new_depth, (int)dag[i].tiny_node[newi[i]].depth);
	}

	int cost = dag[0].funcs.cost[fi];

	if (new_depth >= depth+cost) {
	  add(depth+cost, newi);
	}
      }
    }
  }
```
다음의 과정을 통해서 Ice cuber는 단순한 형태에 대해서 쉽게 문제를 풀수 있도록 Preset을 지정하고 이를 바탕으로 Final Predication을 제작해내는데 이는 마치 어린아이가 직소퍼즐 블록을 들고 조립하여 문제를 푸는 과정이과 유사하다고 생각되었다. 이는 법칙을 추론하는 절차라기 보다 많은 try를 바탕으로 가장 빠르게 퍼즐을 조립하는 어린아이의 관점의 지능에서 제작된 알고리즘임을 유추해볼 수 있었다.

### DreamCoder

해당 논문의 원문은 아래에 나와있다.,   
DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning  

이는 전문가의 학습에 대해서 고민한 딥러닝 기법으로 Dream과정과 Sleeping과정이 있으며, Dream의 절차에서는 DSL을 받아들이고 읽어들이는 절차를 진행한다. 현재 에디터에서 

### Dreaming with ARC

현재 공부중 

### ARC-kit

## 종합

현재 아크에서 가장 핫(?)한 주제가 몇 개가 존재한다.

1. 어떻게 연산자를 합칠 것인가 - DAG, Dreaming sleep ,MCTS(둘이 유사), PeARL 
2. 어떻게 최소 pices를 정의하는가 - icecube
3. 어떻게 아크의 문제의 특성을  정의하는가 - ARC HARD, EASY, CNN
4.  어떻게 연산 모델끼리 합칠 것인가 - Ensenble: 무차별 대입, Overlap과 Gain을 적용 후 푸는 법

## 현재 한 것

1. 기존에 존재하는 연산자들의 유형을 정리 할 것 -> 40% clear (arc - dsl 기준으로 공부 중)
2. 기존의 존재하는 연산자들을 묶어서 시도해본것 -> 10% clear (arc 7 solver를 바탕으로 시도중 + ice cuber)
3. 연산자들을 묶는 방법에 대해서 이해하는것 -> 60% clear ( Ensamble과 Dag, dicision tree 공부 진행 및 구현 중, 연산자들에 대해서 지속적으로 공부중)
4. 연산자들을 묶는 방법에 변형을 주어 시도해 보는것 -> 30% clear ( 관련한 코드를 살짝 바꿔서 진행만 해봄
5. AI를 기반 및 LLM 시도해 보는것 -> 0% (아직 계획 없음) 
6. arc-kit 가지고 놀아보기 -> 50% (가벼운 사용법 학습 중)
7. arc 학습 또는 풀어본 내용을 다같이 공유하기 위해서 에디터 제작 ->  70%(현재의 에디터가 그러한 관점에서 진행중임)

